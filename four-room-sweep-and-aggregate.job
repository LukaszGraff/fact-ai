#!/bin/bash

#SBATCH --partition=gpu_a100
#SBATCH --gpus=1
#SBATCH --job-name=fourroom_sweep
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --time=06:00:00
#SBATCH --output=slurm_%x_%A_%a.out

#SBATCH --ear=on
#SBATCH --ear-policy=monitoring
#SBATCH --ear-verbose=1

set -euo pipefail

# ------------------------------------------------------------
# Single-file sweep launcher + worker.
#
# Usage:
#   sbatch four-room-sweep-and-aggregate.job
#
# What it does:
#  1) Launcher mode (default): submits an array of 1000 training jobs
#     and then submits an aggregate visualization job depending on completion.
#  2) Worker mode (MODE=train): trains one seed based on SLURM_ARRAY_TASK_ID.
#  3) Worker mode (MODE=aggregate): aggregates all saved models into one heatmap.
# ------------------------------------------------------------

MODE=${MODE:-launcher}
SWEEP_ID=${SWEEP_ID:-}

module purge
module load 2025
module load Anaconda3/2025.06-1

cd "${SLURM_SUBMIT_DIR:-$PWD}"
source activate fairdice

export MUJOCO_PY_MUJOCO_PATH="$HOME/.mujoco/mujoco210"
export MUJOCO_GL=egl
export D4RL_SUPPRESS_IMPORT_ERROR=1
export LD_LIBRARY_PATH="/usr/lib/nvidia:/usr/lib64/nvidia:$MUJOCO_PY_MUJOCO_PATH/bin:$CONDA_PREFIX/lib:${LD_LIBRARY_PATH:-}"

# --------- Sweep config ---------
ENV_NAME="MO-FourRoom-v2"
QUALITY="amateur"
PREF_DIST="uniform"
DIVERGENCE="SOFT_CHI"
BETA="0.01"
GAMMA="0.99"
BATCH_SIZE="128"
HIDDEN_DIM="128"
NUM_LAYERS="3"
TOTAL_TRAIN_STEPS="20000"
LOG_INTERVAL="1000"
EVAL_EPISODES="10"
MAX_SEQ_LEN="400"
POLICY_LR="1e-4"
NU_LR="1e-4"
MU_LR="1e-4"
NORMALIZE_REWARD="True"
SAVE_MODEL_MODE="last"   # best_nsw or last

# Data generation (run once before sweep)
GENERATE_DATA=${GENERATE_DATA:-0}
NUM_TRAJ=${NUM_TRAJ:-3000}
DATA_MAX_STEPS=${DATA_MAX_STEPS:-400}
BEHAVIOR=${BEHAVIOR:-greedy_mix}
GOAL_MIX=${GOAL_MIX:-0.6,0.2,0.2}
EPS_GREEDY=${EPS_GREEDY:-0.3}

# Aggregate visualization
EPISODES_PER_MODEL=${EPISODES_PER_MODEL:-3}
AGG_MAX_STEPS=${AGG_MAX_STEPS:-100}
AGG_STOCHASTIC=${AGG_STOCHASTIC:-1}   # 1 to sample actions, 0 for greedy

# Parallelism for the array
ARRAY_SIZE=${ARRAY_SIZE:-1000}
ARRAY_PARALLELISM=${ARRAY_PARALLELISM:-50}

if [[ "$MODE" == "launcher" ]]; then
  if [[ -z "$SWEEP_ID" ]]; then
    SWEEP_ID="sweep_$(date +%Y%m%d_%H%M%S)"
  fi

  echo "Launching sweep: $SWEEP_ID"

  if [[ "$GENERATE_DATA" == "1" ]]; then
    echo "Generating offline data once (num_trajectories=$NUM_TRAJ)..."
    python generate_fourroom_data.py \
      --env_name "$ENV_NAME" \
      --num_trajectories "$NUM_TRAJ" \
      --quality "$QUALITY" \
      --preference_dist "$PREF_DIST" \
      --max_steps "$DATA_MAX_STEPS" \
  fi

  TRAIN_JOB_ID=$(sbatch --parsable \
    --array=0-$((ARRAY_SIZE-1))%$ARRAY_PARALLELISM \
    --export=ALL,MODE=train,SWEEP_ID="$SWEEP_ID" \
    four-room-sweep-and-aggregate.job)

  echo "Submitted training array job: $TRAIN_JOB_ID"

  VIS_JOB_ID=$(sbatch --parsable \
    --dependency=afterok:${TRAIN_JOB_ID} \
    --export=ALL,MODE=aggregate,SWEEP_ID="$SWEEP_ID" \
    four-room-sweep-and-aggregate.job)

  echo "Submitted aggregate job: $VIS_JOB_ID (depends on $TRAIN_JOB_ID)"
  echo "Sweep directory: ./results/$SWEEP_ID"
  exit 0
fi

if [[ -z "$SWEEP_ID" ]]; then
  echo "SWEEP_ID must be set (launcher sets it automatically)." >&2
  exit 1
fi

RESULTS_DIR="./results/$SWEEP_ID"
mkdir -p "$RESULTS_DIR"

if [[ "$MODE" == "train" ]]; then
  SEED=${SLURM_ARRAY_TASK_ID:-0}
  RUN_NAME="seed_${SEED}"

  echo "Training seed=$SEED -> $RESULTS_DIR/$RUN_NAME"

  python main_fourroom.py \
    --mode train \
    --env_name "$ENV_NAME" \
    --learner FairDICE \
    --quality "$QUALITY" \
    --preference_dist "$PREF_DIST" \
    --divergence "$DIVERGENCE" \
    --beta "$BETA" \
    --gamma "$GAMMA" \
    --batch_size "$BATCH_SIZE" \
    --hidden_dim "$HIDDEN_DIM" \
    --num_layers "$NUM_LAYERS" \
    --total_train_steps "$TOTAL_TRAIN_STEPS" \
    --log_interval "$LOG_INTERVAL" \
    --eval_episodes "$EVAL_EPISODES" \
    --max_seq_len "$MAX_SEQ_LEN" \
    --policy_lr "$POLICY_LR" \
    --nu_lr "$NU_LR" \
    --mu_lr "$MU_LR" \
    --normalize_reward "$NORMALIZE_REWARD" \
    --seed "$SEED" \
    --save_path "$RESULTS_DIR" \
    --run_name "$RUN_NAME" \
    --save_model_mode "$SAVE_MODEL_MODE"

  exit 0
fi

if [[ "$MODE" == "aggregate" ]]; then
  echo "Aggregating models from $RESULTS_DIR"

  STOCH_FLAG=""
  if [[ "$AGG_STOCHASTIC" == "1" ]]; then
    STOCH_FLAG="--stochastic"
  fi

  python visualize_fourroom.py \
    --sweep_dir "$RESULTS_DIR" \
    --env_name "$ENV_NAME" \
    --episodes_per_model "$EPISODES_PER_MODEL" \
    --max_steps "$AGG_MAX_STEPS" \
    $STOCH_FLAG \
    --output "$RESULTS_DIR/aggregate_policy_heatmap.png"

  echo "Done. Output: $RESULTS_DIR/aggregate_policy_heatmap.png"
  exit 0
fi

echo "Unknown MODE=$MODE" >&2
exit 1
